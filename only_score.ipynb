{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f160c7f3-8c05-4cfb-bc46-f63831688b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#一定要先，不然torch會偵測不到\n",
    "!export CUDA_VISIBLE_DEVICES=4\n",
    "%set_env CUDA_VISIBLE_DEVICES=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca98d0cd-2c83-4e65-84f7-838804e11822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, BertConfig\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "from tqdm import tqdm\n",
    "from data.overall_dataloader import CustomDataset\n",
    "from model.model_overall import multiBert\n",
    "from data.scale import get_scaled_down_scores, separate_and_rescale_attributes_for_scoring\n",
    "from utils.evaluate import evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461acc45-8d22-44c8-ad9f-ddeccc0fb6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(11)\n",
    "\n",
    "class NerConfig:\n",
    "    def __init__(self):\n",
    "        self.lr = 1e-3\n",
    "        self.epoch = 10\n",
    "        self.batch_size = 3\n",
    "        self.device = \"cuda\"\n",
    "        # self.chunk_sizes = [90]\n",
    "        self.chunk_sizes = [90, 30, 130, 10]\n",
    "        self.data_file = \"/home/tsaibw/Multi_scale/ckps/only_score_V2\"\n",
    "args = NerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a7205-98d8-463e-9431-d405f3ddfd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/protact/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Epoch 1/10: 100% 3166/3166 [30:54<00:00,  1.71it/s]\n",
      "/opt/miniconda/envs/protact/lib/python3.8/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson :  nan\n",
      "QWK :  0.0\n",
      "Epoch 0, Train Loss: 0.3720087448484485\n",
      "Test Loss: 0.5212500157828133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100% 3166/3166 [30:45<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson :  nan\n",
      "QWK :  0.0\n",
      "Epoch 1, Train Loss: 0.371555737275642\n",
      "Test Loss: 0.5212500157828133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100% 3166/3166 [30:29<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson :  nan\n",
      "QWK :  0.0\n",
      "Epoch 2, Train Loss: 0.37155827913087375\n",
      "Test Loss: 0.5212500157828133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100% 3166/3166 [30:35<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson :  nan\n",
      "QWK :  0.0\n",
      "Epoch 3, Train Loss: 0.3715609271933554\n",
      "Test Loss: 0.5212500157828133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100% 3166/3166 [30:28<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson :  nan\n",
      "QWK :  0.0\n",
      "Epoch 4, Train Loss: 0.37156680341402865\n",
      "Test Loss: 0.5212500157828133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  15% 482/3166 [04:39<26:13,  1.71it/s]"
     ]
    }
   ],
   "source": [
    "# train normalize\n",
    "# overall\n",
    "\n",
    "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def print_gradients(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.grad is not None:\n",
    "            print(f\"{name} - Gradient Norm: {parameter.grad.norm().item()}\")\n",
    "        else:\n",
    "            print(f\"{name} - No gradient\")\n",
    "\n",
    "\n",
    "for i in range(6,9):\n",
    "    multi_bert_model = multiBert(args.chunk_sizes)  \n",
    "    multi_bert_model.to(args.device)  \n",
    "    optimizer = Adam(multi_bert_model.parameters(), lr = args.lr) \n",
    "    \n",
    "    train_dataset = CustomDataset(f\"/home/tsaibw/Multi_scale/dataset/new_train/encode_prompt_{i}.pkl\")\n",
    "    eval_dataset = CustomDataset(f\"/home/tsaibw/Multi_scale/dataset/new_test/encode_prompt_{i}.pkl\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    train_loss_list , eval_loss_list = [] ,[] \n",
    "    os.makedirs(f\"{args.data_file}/prompt{i}\", exist_ok=True)\n",
    "    accumulation_steps = 4\n",
    "    for epoch in range(args.epoch):\n",
    "        multi_bert_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (document_single, chunked_documents, label, id_, lengths, readability, hand_craft) in enumerate (tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{args.epoch}\")):\n",
    "            document_single = document_single.to(args.device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predictions = multi_bert_model(\n",
    "                    document_single=document_single,\n",
    "                    chunked_documents=chunked_documents,\n",
    "                    device=args.device,\n",
    "                    lengths=lengths,\n",
    "                    readability = readability.to(args.device),\n",
    "                    hand_craft = hand_craft.to(args.device)\n",
    "            )\n",
    "            \n",
    "            # loss, inverse_predictions, inverse_labels = multi_bert_model.compute_loss(predictions, label, id_, args.device)\n",
    "            # total_loss += loss.item()\n",
    "\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            loss, _, _ = multi_bert_model.compute_loss(predictions, label, id_, args.device)\n",
    "            loss = loss / accumulation_steps  \n",
    "            loss.backward()  \n",
    "            total_loss += loss.item() * accumulation_steps  \n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "        eval_loss, qwk_score, pearson_score = multi_bert_model.evaluate(eval_loader, device = args.device)\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Train Loss: {total_loss / len(train_loader)}\")\n",
    "        print(f\"Test Loss: {eval_loss}\")\n",
    "        train_loss_list.append(total_loss / len(train_loader))\n",
    "        eval_loss_list.append(eval_loss)\n",
    "\n",
    "        qwk_path = f\"{args.data_file}/prompt{i}/result.txt\"\n",
    "        with open(qwk_path, \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch + 1}/{args.epoch}, QWK: {qwk_score}, Pearson: {pearson_score}, train_loss: {train_loss_list[-1]}, eval_loss: {eval_loss_list[-1]}\\n\")\n",
    "  \n",
    "        checkpoint_path = f\"{args.data_file}/prompt{i}/epoch_{epoch+1}_checkpoint.pth.tar\"\n",
    "        save_checkpoint({\n",
    "          'epoch': epoch + 1,\n",
    "          'state_dict': multi_bert_model.state_dict(),\n",
    "          'optimizer': optimizer.state_dict(),\n",
    "          'train_loss': total_loss / len(train_loader),\n",
    "          'eval_loss': eval_loss\n",
    "        }, filename = checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013010f-45bd-4dd2-af0f-42057cfb2125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env ProTact",
   "language": "python",
   "name": "protact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
