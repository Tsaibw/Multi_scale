{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4af431-5cea-4016-838d-4021fcc92bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!export CUDA_VISIBLE_DEVICES=4\n",
    "%set_env CUDA_VISIBLE_DEVICES=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b167fa-d364-4172-9613-5ac05c2c4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, BertConfig\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "from tqdm import tqdm\n",
    "from data.dataloader import CustomDataset\n",
    "from model.multi_bert import multiBert\n",
    "from data.scale import get_scaled_down_scores, separate_and_rescale_attributes_for_scoring\n",
    "from utils.evaluate import evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e81446-8d44-4ad5-91f4-3a0d29918e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(11)\n",
    "\n",
    "class NerConfig:\n",
    "    def __init__(self):\n",
    "        self.lr = 1e-5\n",
    "        self.epoch = 15\n",
    "        self.batch_size = 16\n",
    "        self.device = \"cuda\"\n",
    "        self.chunk_sizes = [90]\n",
    "        # self.chunk_sizes = [90, 30, 130, 10]\n",
    "        self.data_file = \"/home/tsaibw/Multi_scale/ckps/chunk_90\"\n",
    "args = NerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6d41f-1c24-4517-ae1b-bdbb2858ee50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/protact/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Epoch 1/15:  43% 240/560 [04:31<05:54,  1.11s/it]"
     ]
    }
   ],
   "source": [
    "# train normalize\n",
    "\n",
    "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def print_gradients(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.grad is not None:\n",
    "            print(f\"{name} - Gradient Norm: {parameter.grad.norm().item()}\")\n",
    "        else:\n",
    "            print(f\"{name} - No gradient\")\n",
    "\n",
    "\n",
    "for i in range(1,9):\n",
    "    multi_bert_model = multiBert(args.chunk_sizes)  \n",
    "    multi_bert_model.to(args.device)  \n",
    "    optimizer = Adam(multi_bert_model.parameters(), lr = args.lr) \n",
    "    \n",
    "    train_dataset = CustomDataset(f\"/home/tsaibw/Multi_scale/dataset/train/encode_prompt_{i}.pkl\")\n",
    "    eval_dataset = CustomDataset(f\"/home/tsaibw/Multi_scale/dataset/test/encode_prompt_{i}.pkl\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    train_loss_list , eval_loss_list = [] ,[] \n",
    "    os.makedirs(f\"{args.data_file}/prompt{i}\", exist_ok=True)\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        multi_bert_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for document_single, chunked_documents, label, id_, lengths in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{args.epoch}\"):\n",
    "            document_single = document_single.to(args.device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predictions = multi_bert_model(\n",
    "                    document_single=document_single,\n",
    "                    chunked_documents=chunked_documents,\n",
    "                    device=args.device,\n",
    "                    lengths=lengths\n",
    "            )\n",
    "            \n",
    "            loss, inverse_predictions, inverse_labels = multi_bert_model.compute_loss(predictions, label, id_, args.device)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        eval_loss, qwk_score, pearson_score = multi_bert_model.evaluate(eval_loader, device = args.device)\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Train Loss: {total_loss / len(train_loader)}\")\n",
    "        print(f\"Test Loss: {eval_loss}\")\n",
    "        train_loss_list.append(total_loss / len(train_loader))\n",
    "        eval_loss_list.append(eval_loss)\n",
    "\n",
    "        qwk_path = f\"{args.data_file}/prompt{i}/result.txt\"\n",
    "        with open(qwk_path, \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch + 1}/{args.epoch}, QWK: {qwk_score}, Pearson: {pearson_score}, train_loss: {train_loss_list[-1]}, eval_loss: {eval_loss_list[-1]}\\n\")\n",
    "  \n",
    "        checkpoint_path = f\"{args.data_file}/prompt{i}/epoch_{epoch+1}_checkpoint.pth.tar\"\n",
    "        save_checkpoint({\n",
    "          'epoch': epoch + 1,\n",
    "          'state_dict': multi_bert_model.state_dict(),\n",
    "          'optimizer': optimizer.state_dict(),\n",
    "          'train_loss': total_loss / len(train_loader),\n",
    "          'eval_loss': eval_loss\n",
    "        }, filename = checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8b32a-d5b1-401b-8977-dd7912cee561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env ProTact",
   "language": "python",
   "name": "protact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
