{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4af431-5cea-4016-838d-4021fcc92bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#一定要先，不然torch會偵測不到\n",
    "!export CUDA_VISIBLE_DEVICES=3\n",
    "%set_env CUDA_VISIBLE_DEVICES=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b167fa-d364-4172-9613-5ac05c2c4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset, Subset\n",
    "from transformers import BertModel, BertConfig\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "from tqdm import tqdm\n",
    "from data.dataloader import CustomDataset\n",
    "from model.multi_bert import multiBert\n",
    "from data.scale import get_scaled_down_scores, separate_and_rescale_attributes_for_scoring\n",
    "from utils.multitask_evaluator_all_attributes import Evaluator\n",
    "from torch.utils.data import random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e81446-8d44-4ad5-91f4-3a0d29918e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(11)\n",
    "\n",
    "class NerConfig:\n",
    "    def __init__(self):\n",
    "        self.lr = 1e-5\n",
    "        self.epoch = 15\n",
    "        self.batch_size = 10\n",
    "        self.device = \"cuda\"\n",
    "        self.num_trait = 9\n",
    "        self.alpha = 0.7\n",
    "        self.delta = 0.7\n",
    "        self.filter_num = 100\n",
    "        self.chunk_sizes = [90, 30, 130, 10]\n",
    "        self.data_file = \"/home/tsaibw/Multi_scale/ckps/feacture\"\n",
    "        self.hidden_dim = 100\n",
    "        self.mhd_head = 2\n",
    "args = NerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6123e3e-b5df-4546-b447-ce5b7fab109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(f\"/home/tsaibw/Multi_scale/dataset/new_train/encode_prompt_1.pkl\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a5b6b3e-5443-4d20-bd5f-38558b1effbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_id: torch.Size([10])\n",
      "document_single: torch.Size([10, 3, 3, 512])\n",
      "chunked_documents: <class 'list'>\n",
      "length: <class 'list'>\n",
      "hand_craft: torch.Size([10, 52])\n",
      "readability: torch.Size([10, 34])\n",
      "scaled_score: torch.Size([10, 9])\n",
      "torch.Size([10, 17, 3, 90])\n",
      "torch.Size([10, 53, 3, 30])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for key in batch.keys():\n",
    "        value = batch[key]\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"{key}: {value.shape}\")  # 打印張量的形狀\n",
    "        else:\n",
    "            print(f\"{key}: {type(value)}\")  # 如果不是張量，打印類型\n",
    "\n",
    "    print(batch['chunked_documents'][0].shape)\n",
    "    print(batch['chunked_documents'][1].shape)\n",
    "    print(batch['length'][0].shape)\n",
    "    break  # 測試一個批次即可\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b6d41f-1c24-4517-ae1b-bdbb2858ee50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0% 0/953 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 64\u001b[0m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     53\u001b[0m loss, predict_score, scaled_score \u001b[38;5;241m=\u001b[39m multi_bert_model(\n\u001b[1;32m     54\u001b[0m         prompt_id \u001b[38;5;241m=\u001b[39m prompt_id,\n\u001b[1;32m     55\u001b[0m         document_single\u001b[38;5;241m=\u001b[39mdocument_single,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m         scaled_score \u001b[38;5;241m=\u001b[39m scaled_score\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 64\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     65\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "# train normalize\n",
    "# multi trait\n",
    "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def print_gradients(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.grad is not None:\n",
    "            print(f\"{name} - Gradient Norm: {parameter.grad.norm().item()}\")\n",
    "        else:\n",
    "            print(f\"{name} - No gradient\")\n",
    "\n",
    "\n",
    "def get_reduced_dataset(dataset, ratio=0.1):\n",
    "    dataset_size = len(dataset)\n",
    "    reduced_size = int(dataset_size * ratio)\n",
    "    return Subset(dataset, range(reduced_size))  \n",
    "\n",
    "\n",
    "for i in range(4,9):\n",
    "    multi_bert_model = multiBert(args)  \n",
    "    multi_bert_model.to(args.device)  \n",
    "    optimizer = Adam(multi_bert_model.parameters(), lr = args.lr)\n",
    "    train_dataset = CustomDataset(f\"/home/tsaibw/Multi_scale/dataset/new_train/encode_prompt_{i}.pkl\")\n",
    "    eval_dataset = CustomDataset(f\"/home/tsaibw/Multi_scale/dataset/new_dev/encode_prompt_{i}.pkl\")\n",
    "    test_dataset = CustomDataset(f\"/home/tsaibw/Multi_scale/dataset/new_test/encode_prompt_{i}.pkl\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "    dev_loader = DataLoader(eval_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "    evaluator = Evaluator(eval_dataset, test_dataset, 11)\n",
    "    \n",
    "    train_loss_list , eval_loss_list, test_loss_list = [] ,[] , []\n",
    "    os.makedirs(f\"{args.data_file}/prompt{i}\", exist_ok=True)\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        multi_bert_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{args.epoch}\"):\n",
    "            document_single = batch.get(\"document_single\")\n",
    "            chunked_documents = batch.get(\"chunked_documents\")\n",
    "            scaled_score = batch.get(\"scaled_score\")\n",
    "            prompt_id = batch.get(\"prompt_id\")\n",
    "            lengths = batch.get(\"lengths\")\n",
    "            readability = batch.get(\"readability\")\n",
    "            hand_craft = batch.get(\"hand_craft\")\n",
    "\n",
    "            document_single = document_single.to(args.device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss, predict_score, scaled_score = multi_bert_model(\n",
    "                    prompt_id = prompt_id,\n",
    "                    document_single=document_single,\n",
    "                    chunked_documents=chunked_documents,\n",
    "                    device=args.device,\n",
    "                    lengths=lengths,\n",
    "                    readability = readability.to(args.device),\n",
    "                    hand_craft = hand_craft.to(args.device),\n",
    "                    scaled_score = scaled_score\n",
    "            )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # eval_loader = eval_dataset + test_dataset\n",
    "        eval_loss, test_loss, result = multi_bert_model.evaluate(dev_loader, test_loader, epoch, evaluator, device=args.device)\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Train Loss: {total_loss / len(train_loader)}\")\n",
    "        print(f\"Eval Loss: {eval_loss}\")\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "        train_loss_list.append(total_loss / len(train_loader))\n",
    "        eval_loss_list.append(eval_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        \n",
    "        qwk_path = f\"{args.data_file}/prompt{i}/result.txt\"\n",
    "        with open(qwk_path, \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch + 1}/{args.epoch}, result:{result}, train_loss: {train_loss_list[-1]}, eval_loss: {eval_loss_list[-1]}, test_loss: {test_loss_list[-1]} \\n\")\n",
    "  \n",
    "        checkpoint_path = f\"{args.data_file}/prompt{i}/epoch_{epoch+1}_checkpoint.pth.tar\"\n",
    "        save_checkpoint({\n",
    "          'epoch': epoch + 1,\n",
    "          'state_dict': multi_bert_model.state_dict(),\n",
    "          'optimizer': optimizer.state_dict(),\n",
    "          'train_loss': total_loss / len(train_loader),\n",
    "          'eval_loss': eval_loss\n",
    "        }, filename = checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a4faf-ec3a-47f6-9d40-760336cf088f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87138c19-ad02-4e0e-beca-8deeb7ebf9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env ProTact",
   "language": "python",
   "name": "protact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
